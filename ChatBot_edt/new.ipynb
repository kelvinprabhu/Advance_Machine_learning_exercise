{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \\\n",
    "#     \"pinecone\" \\\n",
    "#     \"langchain-chromadb\" \\\n",
    "#     \"langchain-openai\" \\\n",
    "#     \"langchain-text-splitters\" \\\n",
    "#     \"langchain\" \\\n",
    "#     \"langchain-embeddings\" \\\n",
    "#     \"python-frontmatter\" \\\n",
    "#     \"PyPDF2\" \\\n",
    "#     \"sklearn\"\\\n",
    "#     \"langchain_community\"\\\n",
    "#     \"\"\\\n",
    "#     \"\"\\\n",
    "#     \"\"\\\n",
    "#     \"\"\\\n",
    "# %pip install -qU langchain-text-splitters\n",
    "# %pip install -qU langchain-ollama\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown file created at D:\\MSAIM\\trimister-2 msaiml\\Adv-Machine_learning\\ChatBot_edt\\md\\demo.md\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "import frontmatter\n",
    "\n",
    "def pdf_to_md(pdf_path, md_path):\n",
    "    # Load the PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = documents[0].metadata if documents else {}\n",
    "    \n",
    "    # Extract content\n",
    "    content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    # Create frontmatter metadata block\n",
    "    post = frontmatter.Post(content)\n",
    "    for key, value in metadata.items():\n",
    "        post[key] = value\n",
    "    \n",
    "    # Write to Markdown file\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as md_file:\n",
    "        md_file.write(frontmatter.dumps(post))\n",
    "    \n",
    "    print(f\"Markdown file created at {md_path}\")\n",
    "\n",
    "# Input and output paths\n",
    "pdf_path = r\"D:\\MSAIM\\trimister-2 msaiml\\Adv-Machine_learning\\ChatBot_edt\\pdf\\The Subtle Art of Not Giving a F_ck-5-99.pdf\"  # Replace with your PDF file path\n",
    "md_path = r\"D:\\MSAIM\\trimister-2 msaiml\\Adv-Machine_learning\\ChatBot_edt\\md\\demo.md\"    # Replace with desired Markdown file path\n",
    "\n",
    "pdf_to_md(pdf_path, md_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "with open(r\"D:\\MSAIM\\trimister-2 msaiml\\Adv-Machine_learning\\ChatBot_edt\\md\\demo.md\", encoding=\"utf-8\") as f:\n",
    "    sangf = f.read()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=500,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    "\n",
    ")\n",
    "# chunks = text_splitter.split_documents(sangf)\n",
    "texts = text_splitter.create_documents([sangf])\n",
    "# sangf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='---\n",
      "page: 0\n",
      "source: D:\\MSAIM\\trimister-2 msaiml\\Adv-Machine_learning\\ChatBot_edt\\pdf\\The Subtle\n",
      "  Art of Not Giving a F_ck-5-99.pdf\n",
      "---' metadata={'start_index': 0}\n",
      "page_content='CHAPTER 1\n",
      "Don’t Try\n",
      "Charles Bukowski was an alcoholic, a womanizer, a chronic\n",
      "gambler, a lout, a cheapskate, a deadbeat, and on his worst\n",
      "days, a poet. He’s probably the last person on earth you\n",
      "would ever look to for life advice or expect to see in any sort\n",
      "of self-help book.\n",
      "Which is why he’s the perfect place to start.\n",
      "Bukowski wanted to be a writer. But for decades his work\n",
      "was rejected by almost every magazine, newspaper, journal,\n",
      "agent, and publisher he submitted to. His work was horrible,\n",
      "they said. Crude. Disgusting. Depraved. And as the stacks of\n",
      "rejection slips piled up, the weight of his failures pushed him\n",
      "deep into an alcohol-fueled depression that would follow\n",
      "him for most of his life.\n",
      "Bukowski had a day job as a letter-ﬁler at a post oﬃce.\n",
      "He got paid shit money and spent most of it on booze. He\n",
      "gambled away the rest at the racetrack. At night, he would\n",
      "drink alone and sometimes hammer out poetry on his beat-\n",
      "up old typewriter. Often, he’d wake up on the ﬂoor, having' metadata={'start_index': 137}\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "chunks = text_splitter.split_text(sangf)\n",
    "\n",
    "embed_model_ollama = embedding_functions.OllamaEmbeddingFunction(\n",
    "    url=\"http://localhost:11434/api/embeddings\",\n",
    "    model_name=\"llama3.2\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Using cached torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.0/1.7 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 3.1 MB/s eta 0:00:00\n",
      "Installing collected packages: networkx, torch\n",
      "Successfully installed networkx-3.4.2 torch-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 3.3.1 requires Pillow, which is not installed.\n",
      "sentence-transformers 3.3.1 requires scikit-learn, which is not installed.\n",
      "sentence-transformers 3.3.1 requires scipy, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:397\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\versions.py:102\u001b[0m, in \u001b[0;36mrequire_version\u001b[1;34m(requirement, hint)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     got_ver \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:889\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \n\u001b[0;32m    885\u001b[0m \u001b[38;5;124;03m:param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m:return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m    \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:862\u001b[0m, in \u001b[0;36mdistribution\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \n\u001b[0;32m    859\u001b[0m \u001b[38;5;124;03m:param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m:return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:399\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: No package metadata was found for regex",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\embeddings\\huggingface.py:84\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\backend.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_datasets_caching, is_datasets_available\n\u001b[0;32m     13\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\util.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_torch_npu_available\n\u001b[0;32m     24\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\dependency_versions_check.py:57\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\versions.py:117\u001b[0m, in \u001b[0;36mrequire_version_core\u001b[1;34m(requirement)\u001b[0m\n\u001b[0;32m    116\u001b[0m hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: `pip install transformers -U` or `pip install -e \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.[dev]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` if you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre working with git main\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\versions.py:104\u001b[0m, in \u001b[0;36mrequire_version\u001b[1;34m(requirement, hint)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError(\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m distribution was not found and is required by this application. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m     )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# check that the right version is installed if version number or a range was provided\u001b[39;00m\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: No package metadata was found for The 'regex!=2019.12.17' distribution was not found and is required by this application. \nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m embed_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 8\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\embeddings\\huggingface.py:87\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer(\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\n\u001b[0;32m     94\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`."
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 279 doc embeddings, each with a dimensionality of 3072.\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_model_ollama(chunks)\n",
    "\n",
    "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
    "      f\"a dimensionality of {len(embeddings[0])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU: {cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "True\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is enabled\n",
    "print(torch.version.cuda)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chunk-1', 'metadata': {'chunk_id': 1, 'text': 'CHAPTER 1\\nDon’t Try\\nCharles Bukowski was an alcoholic, a womanizer, a chronic\\ngambler, a lout, a cheapskate, a deadbeat, and on his worst\\ndays, a poet. He’s probably the last person on earth you\\nwould ever look to for life advice or expect to see in any sort\\nof self-help book.\\nWhich is why he’s the perfect place to start.\\nBukowski wanted to be a writer. But for decades his work\\nwas rejected by almost every magazine, newspaper, journal,\\nagent, and publisher he submitted to. His work was horrible,\\nthey said. Crude. Disgusting. Depraved. And as the stacks of\\nrejection slips piled up, the weight of his failures pushed him\\ndeep into an alcohol-fueled depression that would follow\\nhim for most of his life.\\nBukowski had a day job as a letter-ﬁler at a post oﬃce.\\nHe got paid shit money and spent most of it on booze. He\\ngambled away the rest at the racetrack. At night, he would\\ndrink alone and sometimes hammer out poetry on his beat-\\nup old typewriter. Often, he’d wake up on the ﬂoor, having'}, 'embedding': array([-0.66976184, -1.9372813 ,  1.1795187 , ..., -0.41718957,\n",
      "        0.9074979 , -1.4502169 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# Generate metadata\n",
    "metadata = [{\"chunk_id\": i, \"text\": chunk} for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Prepare data for ChromaDB\n",
    "chroma_data = [\n",
    "    {\n",
    "        \"id\": f\"chunk-{i}\",        # Unique ID for the chunk\n",
    "        \"metadata\": metadata[i],  # Metadata dictionary\n",
    "        \"embedding\": embeddings[i]  # Embedding vector\n",
    "    }\n",
    "    for i in range(len(chunks))\n",
    "]\n",
    "\n",
    "# Example: Access a single chunk's data\n",
    "print(chroma_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: chunk-0\n",
      "Insert of existing embedding ID: chunk-0\n",
      "Add of existing embedding ID: chunk-1\n",
      "Insert of existing embedding ID: chunk-1\n",
      "Add of existing embedding ID: chunk-2\n",
      "Insert of existing embedding ID: chunk-2\n",
      "Add of existing embedding ID: chunk-3\n",
      "Insert of existing embedding ID: chunk-3\n",
      "Add of existing embedding ID: chunk-4\n",
      "Insert of existing embedding ID: chunk-4\n",
      "Add of existing embedding ID: chunk-5\n",
      "Insert of existing embedding ID: chunk-5\n",
      "Add of existing embedding ID: chunk-6\n",
      "Insert of existing embedding ID: chunk-6\n",
      "Add of existing embedding ID: chunk-7\n",
      "Insert of existing embedding ID: chunk-7\n",
      "Add of existing embedding ID: chunk-8\n",
      "Insert of existing embedding ID: chunk-8\n",
      "Add of existing embedding ID: chunk-9\n",
      "Insert of existing embedding ID: chunk-9\n",
      "Add of existing embedding ID: chunk-10\n",
      "Insert of existing embedding ID: chunk-10\n",
      "Add of existing embedding ID: chunk-11\n",
      "Insert of existing embedding ID: chunk-11\n",
      "Add of existing embedding ID: chunk-12\n",
      "Insert of existing embedding ID: chunk-12\n",
      "Add of existing embedding ID: chunk-13\n",
      "Insert of existing embedding ID: chunk-13\n",
      "Add of existing embedding ID: chunk-14\n",
      "Insert of existing embedding ID: chunk-14\n",
      "Add of existing embedding ID: chunk-15\n",
      "Insert of existing embedding ID: chunk-15\n",
      "Add of existing embedding ID: chunk-16\n",
      "Insert of existing embedding ID: chunk-16\n",
      "Add of existing embedding ID: chunk-17\n",
      "Insert of existing embedding ID: chunk-17\n",
      "Add of existing embedding ID: chunk-18\n",
      "Insert of existing embedding ID: chunk-18\n",
      "Add of existing embedding ID: chunk-19\n",
      "Insert of existing embedding ID: chunk-19\n",
      "Add of existing embedding ID: chunk-20\n",
      "Insert of existing embedding ID: chunk-20\n",
      "Add of existing embedding ID: chunk-21\n",
      "Insert of existing embedding ID: chunk-21\n",
      "Add of existing embedding ID: chunk-22\n",
      "Insert of existing embedding ID: chunk-22\n",
      "Add of existing embedding ID: chunk-23\n",
      "Insert of existing embedding ID: chunk-23\n",
      "Add of existing embedding ID: chunk-24\n",
      "Insert of existing embedding ID: chunk-24\n",
      "Add of existing embedding ID: chunk-25\n",
      "Insert of existing embedding ID: chunk-25\n",
      "Add of existing embedding ID: chunk-26\n",
      "Insert of existing embedding ID: chunk-26\n",
      "Add of existing embedding ID: chunk-27\n",
      "Insert of existing embedding ID: chunk-27\n",
      "Add of existing embedding ID: chunk-28\n",
      "Insert of existing embedding ID: chunk-28\n",
      "Add of existing embedding ID: chunk-29\n",
      "Insert of existing embedding ID: chunk-29\n",
      "Add of existing embedding ID: chunk-30\n",
      "Insert of existing embedding ID: chunk-30\n",
      "Add of existing embedding ID: chunk-31\n",
      "Insert of existing embedding ID: chunk-31\n",
      "Add of existing embedding ID: chunk-32\n",
      "Insert of existing embedding ID: chunk-32\n",
      "Add of existing embedding ID: chunk-33\n",
      "Insert of existing embedding ID: chunk-33\n",
      "Add of existing embedding ID: chunk-34\n",
      "Insert of existing embedding ID: chunk-34\n",
      "Add of existing embedding ID: chunk-35\n",
      "Insert of existing embedding ID: chunk-35\n",
      "Add of existing embedding ID: chunk-36\n",
      "Insert of existing embedding ID: chunk-36\n",
      "Add of existing embedding ID: chunk-37\n",
      "Insert of existing embedding ID: chunk-37\n",
      "Add of existing embedding ID: chunk-38\n",
      "Insert of existing embedding ID: chunk-38\n",
      "Add of existing embedding ID: chunk-39\n",
      "Insert of existing embedding ID: chunk-39\n",
      "Add of existing embedding ID: chunk-40\n",
      "Insert of existing embedding ID: chunk-40\n",
      "Add of existing embedding ID: chunk-41\n",
      "Insert of existing embedding ID: chunk-41\n",
      "Add of existing embedding ID: chunk-42\n",
      "Insert of existing embedding ID: chunk-42\n",
      "Add of existing embedding ID: chunk-43\n",
      "Insert of existing embedding ID: chunk-43\n",
      "Add of existing embedding ID: chunk-44\n",
      "Insert of existing embedding ID: chunk-44\n",
      "Add of existing embedding ID: chunk-45\n",
      "Insert of existing embedding ID: chunk-45\n",
      "Add of existing embedding ID: chunk-46\n",
      "Insert of existing embedding ID: chunk-46\n",
      "Add of existing embedding ID: chunk-47\n",
      "Insert of existing embedding ID: chunk-47\n",
      "Add of existing embedding ID: chunk-48\n",
      "Insert of existing embedding ID: chunk-48\n",
      "Add of existing embedding ID: chunk-49\n",
      "Insert of existing embedding ID: chunk-49\n",
      "Add of existing embedding ID: chunk-50\n",
      "Insert of existing embedding ID: chunk-50\n",
      "Add of existing embedding ID: chunk-51\n",
      "Insert of existing embedding ID: chunk-51\n",
      "Add of existing embedding ID: chunk-52\n",
      "Insert of existing embedding ID: chunk-52\n",
      "Add of existing embedding ID: chunk-53\n",
      "Insert of existing embedding ID: chunk-53\n",
      "Add of existing embedding ID: chunk-54\n",
      "Insert of existing embedding ID: chunk-54\n",
      "Add of existing embedding ID: chunk-55\n",
      "Insert of existing embedding ID: chunk-55\n",
      "Add of existing embedding ID: chunk-56\n",
      "Insert of existing embedding ID: chunk-56\n",
      "Add of existing embedding ID: chunk-57\n",
      "Insert of existing embedding ID: chunk-57\n",
      "Add of existing embedding ID: chunk-58\n",
      "Insert of existing embedding ID: chunk-58\n",
      "Add of existing embedding ID: chunk-59\n",
      "Insert of existing embedding ID: chunk-59\n",
      "Add of existing embedding ID: chunk-60\n",
      "Insert of existing embedding ID: chunk-60\n",
      "Add of existing embedding ID: chunk-61\n",
      "Insert of existing embedding ID: chunk-61\n",
      "Add of existing embedding ID: chunk-62\n",
      "Insert of existing embedding ID: chunk-62\n",
      "Add of existing embedding ID: chunk-63\n",
      "Insert of existing embedding ID: chunk-63\n",
      "Add of existing embedding ID: chunk-64\n",
      "Insert of existing embedding ID: chunk-64\n",
      "Add of existing embedding ID: chunk-65\n",
      "Insert of existing embedding ID: chunk-65\n",
      "Add of existing embedding ID: chunk-66\n",
      "Insert of existing embedding ID: chunk-66\n",
      "Add of existing embedding ID: chunk-67\n",
      "Insert of existing embedding ID: chunk-67\n",
      "Add of existing embedding ID: chunk-68\n",
      "Insert of existing embedding ID: chunk-68\n",
      "Add of existing embedding ID: chunk-69\n",
      "Insert of existing embedding ID: chunk-69\n",
      "Add of existing embedding ID: chunk-70\n",
      "Insert of existing embedding ID: chunk-70\n",
      "Add of existing embedding ID: chunk-71\n",
      "Insert of existing embedding ID: chunk-71\n",
      "Add of existing embedding ID: chunk-72\n",
      "Insert of existing embedding ID: chunk-72\n",
      "Add of existing embedding ID: chunk-73\n",
      "Insert of existing embedding ID: chunk-73\n",
      "Add of existing embedding ID: chunk-74\n",
      "Insert of existing embedding ID: chunk-74\n",
      "Add of existing embedding ID: chunk-75\n",
      "Insert of existing embedding ID: chunk-75\n",
      "Add of existing embedding ID: chunk-76\n",
      "Insert of existing embedding ID: chunk-76\n",
      "Add of existing embedding ID: chunk-77\n",
      "Insert of existing embedding ID: chunk-77\n",
      "Add of existing embedding ID: chunk-78\n",
      "Insert of existing embedding ID: chunk-78\n",
      "Add of existing embedding ID: chunk-79\n",
      "Insert of existing embedding ID: chunk-79\n",
      "Add of existing embedding ID: chunk-80\n",
      "Insert of existing embedding ID: chunk-80\n",
      "Add of existing embedding ID: chunk-81\n",
      "Insert of existing embedding ID: chunk-81\n",
      "Add of existing embedding ID: chunk-82\n",
      "Insert of existing embedding ID: chunk-82\n",
      "Add of existing embedding ID: chunk-83\n",
      "Insert of existing embedding ID: chunk-83\n",
      "Add of existing embedding ID: chunk-84\n",
      "Insert of existing embedding ID: chunk-84\n",
      "Add of existing embedding ID: chunk-85\n",
      "Insert of existing embedding ID: chunk-85\n",
      "Add of existing embedding ID: chunk-86\n",
      "Insert of existing embedding ID: chunk-86\n",
      "Add of existing embedding ID: chunk-87\n",
      "Insert of existing embedding ID: chunk-87\n",
      "Add of existing embedding ID: chunk-88\n",
      "Insert of existing embedding ID: chunk-88\n",
      "Add of existing embedding ID: chunk-89\n",
      "Insert of existing embedding ID: chunk-89\n",
      "Add of existing embedding ID: chunk-90\n",
      "Insert of existing embedding ID: chunk-90\n",
      "Add of existing embedding ID: chunk-91\n",
      "Insert of existing embedding ID: chunk-91\n",
      "Add of existing embedding ID: chunk-92\n",
      "Insert of existing embedding ID: chunk-92\n",
      "Add of existing embedding ID: chunk-93\n",
      "Insert of existing embedding ID: chunk-93\n",
      "Add of existing embedding ID: chunk-94\n",
      "Insert of existing embedding ID: chunk-94\n",
      "Add of existing embedding ID: chunk-95\n",
      "Insert of existing embedding ID: chunk-95\n",
      "Add of existing embedding ID: chunk-96\n",
      "Insert of existing embedding ID: chunk-96\n",
      "Add of existing embedding ID: chunk-97\n",
      "Insert of existing embedding ID: chunk-97\n",
      "Add of existing embedding ID: chunk-98\n",
      "Insert of existing embedding ID: chunk-98\n",
      "Add of existing embedding ID: chunk-99\n",
      "Insert of existing embedding ID: chunk-99\n",
      "Add of existing embedding ID: chunk-100\n",
      "Insert of existing embedding ID: chunk-100\n",
      "Add of existing embedding ID: chunk-101\n",
      "Insert of existing embedding ID: chunk-101\n",
      "Add of existing embedding ID: chunk-102\n",
      "Insert of existing embedding ID: chunk-102\n",
      "Add of existing embedding ID: chunk-103\n",
      "Insert of existing embedding ID: chunk-103\n",
      "Add of existing embedding ID: chunk-104\n",
      "Insert of existing embedding ID: chunk-104\n",
      "Add of existing embedding ID: chunk-105\n",
      "Insert of existing embedding ID: chunk-105\n",
      "Add of existing embedding ID: chunk-106\n",
      "Insert of existing embedding ID: chunk-106\n",
      "Add of existing embedding ID: chunk-107\n",
      "Insert of existing embedding ID: chunk-107\n",
      "Add of existing embedding ID: chunk-108\n",
      "Insert of existing embedding ID: chunk-108\n",
      "Add of existing embedding ID: chunk-109\n",
      "Insert of existing embedding ID: chunk-109\n",
      "Add of existing embedding ID: chunk-110\n",
      "Insert of existing embedding ID: chunk-110\n",
      "Add of existing embedding ID: chunk-111\n",
      "Insert of existing embedding ID: chunk-111\n",
      "Add of existing embedding ID: chunk-112\n",
      "Insert of existing embedding ID: chunk-112\n",
      "Add of existing embedding ID: chunk-113\n",
      "Insert of existing embedding ID: chunk-113\n",
      "Add of existing embedding ID: chunk-114\n",
      "Insert of existing embedding ID: chunk-114\n",
      "Add of existing embedding ID: chunk-115\n",
      "Insert of existing embedding ID: chunk-115\n",
      "Add of existing embedding ID: chunk-116\n",
      "Insert of existing embedding ID: chunk-116\n",
      "Add of existing embedding ID: chunk-117\n",
      "Insert of existing embedding ID: chunk-117\n",
      "Add of existing embedding ID: chunk-118\n",
      "Insert of existing embedding ID: chunk-118\n",
      "Add of existing embedding ID: chunk-119\n",
      "Insert of existing embedding ID: chunk-119\n",
      "Add of existing embedding ID: chunk-120\n",
      "Insert of existing embedding ID: chunk-120\n",
      "Add of existing embedding ID: chunk-121\n",
      "Insert of existing embedding ID: chunk-121\n",
      "Add of existing embedding ID: chunk-122\n",
      "Insert of existing embedding ID: chunk-122\n",
      "Add of existing embedding ID: chunk-123\n",
      "Insert of existing embedding ID: chunk-123\n",
      "Add of existing embedding ID: chunk-124\n",
      "Insert of existing embedding ID: chunk-124\n",
      "Add of existing embedding ID: chunk-125\n",
      "Insert of existing embedding ID: chunk-125\n",
      "Add of existing embedding ID: chunk-126\n",
      "Insert of existing embedding ID: chunk-126\n",
      "Add of existing embedding ID: chunk-127\n",
      "Insert of existing embedding ID: chunk-127\n",
      "Add of existing embedding ID: chunk-128\n",
      "Insert of existing embedding ID: chunk-128\n",
      "Add of existing embedding ID: chunk-129\n",
      "Insert of existing embedding ID: chunk-129\n",
      "Add of existing embedding ID: chunk-130\n",
      "Insert of existing embedding ID: chunk-130\n",
      "Add of existing embedding ID: chunk-131\n",
      "Insert of existing embedding ID: chunk-131\n",
      "Add of existing embedding ID: chunk-132\n",
      "Insert of existing embedding ID: chunk-132\n",
      "Add of existing embedding ID: chunk-133\n",
      "Insert of existing embedding ID: chunk-133\n",
      "Add of existing embedding ID: chunk-134\n",
      "Insert of existing embedding ID: chunk-134\n",
      "Add of existing embedding ID: chunk-135\n",
      "Insert of existing embedding ID: chunk-135\n",
      "Add of existing embedding ID: chunk-136\n",
      "Insert of existing embedding ID: chunk-136\n",
      "Add of existing embedding ID: chunk-137\n",
      "Insert of existing embedding ID: chunk-137\n",
      "Add of existing embedding ID: chunk-138\n",
      "Insert of existing embedding ID: chunk-138\n",
      "Add of existing embedding ID: chunk-139\n",
      "Insert of existing embedding ID: chunk-139\n",
      "Add of existing embedding ID: chunk-140\n",
      "Insert of existing embedding ID: chunk-140\n",
      "Add of existing embedding ID: chunk-141\n",
      "Insert of existing embedding ID: chunk-141\n",
      "Add of existing embedding ID: chunk-142\n",
      "Insert of existing embedding ID: chunk-142\n",
      "Add of existing embedding ID: chunk-143\n",
      "Insert of existing embedding ID: chunk-143\n",
      "Add of existing embedding ID: chunk-144\n",
      "Insert of existing embedding ID: chunk-144\n",
      "Add of existing embedding ID: chunk-145\n",
      "Insert of existing embedding ID: chunk-145\n",
      "Add of existing embedding ID: chunk-146\n",
      "Insert of existing embedding ID: chunk-146\n",
      "Add of existing embedding ID: chunk-147\n",
      "Insert of existing embedding ID: chunk-147\n",
      "Add of existing embedding ID: chunk-148\n",
      "Insert of existing embedding ID: chunk-148\n",
      "Add of existing embedding ID: chunk-149\n",
      "Insert of existing embedding ID: chunk-149\n",
      "Add of existing embedding ID: chunk-150\n",
      "Insert of existing embedding ID: chunk-150\n",
      "Add of existing embedding ID: chunk-151\n",
      "Insert of existing embedding ID: chunk-151\n",
      "Add of existing embedding ID: chunk-152\n",
      "Insert of existing embedding ID: chunk-152\n",
      "Add of existing embedding ID: chunk-153\n",
      "Insert of existing embedding ID: chunk-153\n",
      "Add of existing embedding ID: chunk-154\n",
      "Insert of existing embedding ID: chunk-154\n",
      "Add of existing embedding ID: chunk-155\n",
      "Insert of existing embedding ID: chunk-155\n",
      "Add of existing embedding ID: chunk-156\n",
      "Insert of existing embedding ID: chunk-156\n",
      "Add of existing embedding ID: chunk-157\n",
      "Insert of existing embedding ID: chunk-157\n",
      "Add of existing embedding ID: chunk-158\n",
      "Insert of existing embedding ID: chunk-158\n",
      "Add of existing embedding ID: chunk-159\n",
      "Insert of existing embedding ID: chunk-159\n",
      "Add of existing embedding ID: chunk-160\n",
      "Insert of existing embedding ID: chunk-160\n",
      "Add of existing embedding ID: chunk-161\n",
      "Insert of existing embedding ID: chunk-161\n",
      "Add of existing embedding ID: chunk-162\n",
      "Insert of existing embedding ID: chunk-162\n",
      "Add of existing embedding ID: chunk-163\n",
      "Insert of existing embedding ID: chunk-163\n",
      "Add of existing embedding ID: chunk-164\n",
      "Insert of existing embedding ID: chunk-164\n",
      "Add of existing embedding ID: chunk-165\n",
      "Insert of existing embedding ID: chunk-165\n",
      "Add of existing embedding ID: chunk-166\n",
      "Insert of existing embedding ID: chunk-166\n",
      "Add of existing embedding ID: chunk-167\n",
      "Insert of existing embedding ID: chunk-167\n",
      "Add of existing embedding ID: chunk-168\n",
      "Insert of existing embedding ID: chunk-168\n",
      "Add of existing embedding ID: chunk-169\n",
      "Insert of existing embedding ID: chunk-169\n",
      "Add of existing embedding ID: chunk-170\n",
      "Insert of existing embedding ID: chunk-170\n",
      "Add of existing embedding ID: chunk-171\n",
      "Insert of existing embedding ID: chunk-171\n",
      "Add of existing embedding ID: chunk-172\n",
      "Insert of existing embedding ID: chunk-172\n",
      "Add of existing embedding ID: chunk-173\n",
      "Insert of existing embedding ID: chunk-173\n",
      "Add of existing embedding ID: chunk-174\n",
      "Insert of existing embedding ID: chunk-174\n",
      "Add of existing embedding ID: chunk-175\n",
      "Insert of existing embedding ID: chunk-175\n",
      "Add of existing embedding ID: chunk-176\n",
      "Insert of existing embedding ID: chunk-176\n",
      "Add of existing embedding ID: chunk-177\n",
      "Insert of existing embedding ID: chunk-177\n",
      "Add of existing embedding ID: chunk-178\n",
      "Insert of existing embedding ID: chunk-178\n",
      "Add of existing embedding ID: chunk-179\n",
      "Insert of existing embedding ID: chunk-179\n",
      "Add of existing embedding ID: chunk-180\n",
      "Insert of existing embedding ID: chunk-180\n",
      "Add of existing embedding ID: chunk-181\n",
      "Insert of existing embedding ID: chunk-181\n",
      "Add of existing embedding ID: chunk-182\n",
      "Insert of existing embedding ID: chunk-182\n",
      "Add of existing embedding ID: chunk-183\n",
      "Insert of existing embedding ID: chunk-183\n",
      "Add of existing embedding ID: chunk-184\n",
      "Insert of existing embedding ID: chunk-184\n",
      "Add of existing embedding ID: chunk-185\n",
      "Insert of existing embedding ID: chunk-185\n",
      "Add of existing embedding ID: chunk-186\n",
      "Insert of existing embedding ID: chunk-186\n",
      "Add of existing embedding ID: chunk-187\n",
      "Insert of existing embedding ID: chunk-187\n",
      "Add of existing embedding ID: chunk-188\n",
      "Insert of existing embedding ID: chunk-188\n",
      "Add of existing embedding ID: chunk-189\n",
      "Insert of existing embedding ID: chunk-189\n",
      "Add of existing embedding ID: chunk-190\n",
      "Insert of existing embedding ID: chunk-190\n",
      "Add of existing embedding ID: chunk-191\n",
      "Insert of existing embedding ID: chunk-191\n",
      "Add of existing embedding ID: chunk-192\n",
      "Insert of existing embedding ID: chunk-192\n",
      "Add of existing embedding ID: chunk-193\n",
      "Insert of existing embedding ID: chunk-193\n",
      "Add of existing embedding ID: chunk-194\n",
      "Insert of existing embedding ID: chunk-194\n",
      "Add of existing embedding ID: chunk-195\n",
      "Insert of existing embedding ID: chunk-195\n",
      "Add of existing embedding ID: chunk-196\n",
      "Insert of existing embedding ID: chunk-196\n",
      "Add of existing embedding ID: chunk-197\n",
      "Insert of existing embedding ID: chunk-197\n",
      "Add of existing embedding ID: chunk-198\n",
      "Insert of existing embedding ID: chunk-198\n",
      "Add of existing embedding ID: chunk-199\n",
      "Insert of existing embedding ID: chunk-199\n",
      "Add of existing embedding ID: chunk-200\n",
      "Insert of existing embedding ID: chunk-200\n",
      "Add of existing embedding ID: chunk-201\n",
      "Insert of existing embedding ID: chunk-201\n",
      "Add of existing embedding ID: chunk-202\n",
      "Insert of existing embedding ID: chunk-202\n",
      "Add of existing embedding ID: chunk-203\n",
      "Insert of existing embedding ID: chunk-203\n",
      "Add of existing embedding ID: chunk-204\n",
      "Insert of existing embedding ID: chunk-204\n",
      "Add of existing embedding ID: chunk-205\n",
      "Insert of existing embedding ID: chunk-205\n",
      "Add of existing embedding ID: chunk-206\n",
      "Insert of existing embedding ID: chunk-206\n",
      "Add of existing embedding ID: chunk-207\n",
      "Insert of existing embedding ID: chunk-207\n",
      "Add of existing embedding ID: chunk-208\n",
      "Insert of existing embedding ID: chunk-208\n",
      "Add of existing embedding ID: chunk-209\n",
      "Insert of existing embedding ID: chunk-209\n",
      "Add of existing embedding ID: chunk-210\n",
      "Insert of existing embedding ID: chunk-210\n",
      "Add of existing embedding ID: chunk-211\n",
      "Insert of existing embedding ID: chunk-211\n",
      "Add of existing embedding ID: chunk-212\n",
      "Insert of existing embedding ID: chunk-212\n",
      "Add of existing embedding ID: chunk-213\n",
      "Insert of existing embedding ID: chunk-213\n",
      "Add of existing embedding ID: chunk-214\n",
      "Insert of existing embedding ID: chunk-214\n",
      "Add of existing embedding ID: chunk-215\n",
      "Insert of existing embedding ID: chunk-215\n",
      "Add of existing embedding ID: chunk-216\n",
      "Insert of existing embedding ID: chunk-216\n",
      "Add of existing embedding ID: chunk-217\n",
      "Insert of existing embedding ID: chunk-217\n",
      "Add of existing embedding ID: chunk-218\n",
      "Insert of existing embedding ID: chunk-218\n",
      "Add of existing embedding ID: chunk-219\n",
      "Insert of existing embedding ID: chunk-219\n",
      "Add of existing embedding ID: chunk-220\n",
      "Insert of existing embedding ID: chunk-220\n",
      "Add of existing embedding ID: chunk-221\n",
      "Insert of existing embedding ID: chunk-221\n",
      "Add of existing embedding ID: chunk-222\n",
      "Insert of existing embedding ID: chunk-222\n",
      "Add of existing embedding ID: chunk-223\n",
      "Insert of existing embedding ID: chunk-223\n",
      "Add of existing embedding ID: chunk-224\n",
      "Insert of existing embedding ID: chunk-224\n",
      "Add of existing embedding ID: chunk-225\n",
      "Insert of existing embedding ID: chunk-225\n",
      "Add of existing embedding ID: chunk-226\n",
      "Insert of existing embedding ID: chunk-226\n",
      "Add of existing embedding ID: chunk-227\n",
      "Insert of existing embedding ID: chunk-227\n",
      "Add of existing embedding ID: chunk-228\n",
      "Insert of existing embedding ID: chunk-228\n",
      "Add of existing embedding ID: chunk-229\n",
      "Insert of existing embedding ID: chunk-229\n",
      "Add of existing embedding ID: chunk-230\n",
      "Insert of existing embedding ID: chunk-230\n",
      "Add of existing embedding ID: chunk-231\n",
      "Insert of existing embedding ID: chunk-231\n",
      "Add of existing embedding ID: chunk-232\n",
      "Insert of existing embedding ID: chunk-232\n",
      "Add of existing embedding ID: chunk-233\n",
      "Insert of existing embedding ID: chunk-233\n",
      "Add of existing embedding ID: chunk-234\n",
      "Insert of existing embedding ID: chunk-234\n",
      "Add of existing embedding ID: chunk-235\n",
      "Insert of existing embedding ID: chunk-235\n",
      "Add of existing embedding ID: chunk-236\n",
      "Insert of existing embedding ID: chunk-236\n",
      "Add of existing embedding ID: chunk-237\n",
      "Insert of existing embedding ID: chunk-237\n",
      "Add of existing embedding ID: chunk-238\n",
      "Insert of existing embedding ID: chunk-238\n",
      "Add of existing embedding ID: chunk-239\n",
      "Insert of existing embedding ID: chunk-239\n",
      "Add of existing embedding ID: chunk-240\n",
      "Insert of existing embedding ID: chunk-240\n",
      "Add of existing embedding ID: chunk-241\n",
      "Insert of existing embedding ID: chunk-241\n",
      "Add of existing embedding ID: chunk-242\n",
      "Insert of existing embedding ID: chunk-242\n",
      "Add of existing embedding ID: chunk-243\n",
      "Insert of existing embedding ID: chunk-243\n",
      "Add of existing embedding ID: chunk-244\n",
      "Insert of existing embedding ID: chunk-244\n",
      "Add of existing embedding ID: chunk-245\n",
      "Insert of existing embedding ID: chunk-245\n",
      "Add of existing embedding ID: chunk-246\n",
      "Insert of existing embedding ID: chunk-246\n",
      "Add of existing embedding ID: chunk-247\n",
      "Insert of existing embedding ID: chunk-247\n",
      "Add of existing embedding ID: chunk-248\n",
      "Insert of existing embedding ID: chunk-248\n",
      "Add of existing embedding ID: chunk-249\n",
      "Insert of existing embedding ID: chunk-249\n",
      "Add of existing embedding ID: chunk-250\n",
      "Insert of existing embedding ID: chunk-250\n",
      "Add of existing embedding ID: chunk-251\n",
      "Insert of existing embedding ID: chunk-251\n",
      "Add of existing embedding ID: chunk-252\n",
      "Insert of existing embedding ID: chunk-252\n",
      "Add of existing embedding ID: chunk-253\n",
      "Insert of existing embedding ID: chunk-253\n",
      "Add of existing embedding ID: chunk-254\n",
      "Insert of existing embedding ID: chunk-254\n",
      "Add of existing embedding ID: chunk-255\n",
      "Insert of existing embedding ID: chunk-255\n",
      "Add of existing embedding ID: chunk-256\n",
      "Insert of existing embedding ID: chunk-256\n",
      "Add of existing embedding ID: chunk-257\n",
      "Insert of existing embedding ID: chunk-257\n",
      "Add of existing embedding ID: chunk-258\n",
      "Insert of existing embedding ID: chunk-258\n",
      "Add of existing embedding ID: chunk-259\n",
      "Insert of existing embedding ID: chunk-259\n",
      "Add of existing embedding ID: chunk-260\n",
      "Insert of existing embedding ID: chunk-260\n",
      "Add of existing embedding ID: chunk-261\n",
      "Insert of existing embedding ID: chunk-261\n",
      "Add of existing embedding ID: chunk-262\n",
      "Insert of existing embedding ID: chunk-262\n",
      "Add of existing embedding ID: chunk-263\n",
      "Insert of existing embedding ID: chunk-263\n",
      "Add of existing embedding ID: chunk-264\n",
      "Insert of existing embedding ID: chunk-264\n",
      "Add of existing embedding ID: chunk-265\n",
      "Insert of existing embedding ID: chunk-265\n",
      "Add of existing embedding ID: chunk-266\n",
      "Insert of existing embedding ID: chunk-266\n",
      "Add of existing embedding ID: chunk-267\n",
      "Insert of existing embedding ID: chunk-267\n",
      "Add of existing embedding ID: chunk-268\n",
      "Insert of existing embedding ID: chunk-268\n",
      "Add of existing embedding ID: chunk-269\n",
      "Insert of existing embedding ID: chunk-269\n",
      "Add of existing embedding ID: chunk-270\n",
      "Insert of existing embedding ID: chunk-270\n",
      "Add of existing embedding ID: chunk-271\n",
      "Insert of existing embedding ID: chunk-271\n",
      "Add of existing embedding ID: chunk-272\n",
      "Insert of existing embedding ID: chunk-272\n",
      "Add of existing embedding ID: chunk-273\n",
      "Insert of existing embedding ID: chunk-273\n",
      "Add of existing embedding ID: chunk-274\n",
      "Insert of existing embedding ID: chunk-274\n",
      "Add of existing embedding ID: chunk-275\n",
      "Insert of existing embedding ID: chunk-275\n",
      "Add of existing embedding ID: chunk-276\n",
      "Insert of existing embedding ID: chunk-276\n",
      "Add of existing embedding ID: chunk-277\n",
      "Insert of existing embedding ID: chunk-277\n",
      "Add of existing embedding ID: chunk-278\n",
      "Insert of existing embedding ID: chunk-278\n"
     ]
    }
   ],
   "source": [
    "from chromadb import Client\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = Client()\n",
    "\n",
    "# Create a collection\n",
    "collection = client.get_or_create_collection(name=\"sangf_rag\")\n",
    "\n",
    "# Upsert data\n",
    "for data in chroma_data:\n",
    "    collection.add(\n",
    "        ids=[data[\"id\"]],\n",
    "        embeddings=[data[\"embedding\"]],\n",
    "        metadatas=[data[\"metadata\"]]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 results. {'ids': [['chunk-180', 'chunk-71', 'chunk-9']], 'embeddings': None, 'documents': [[None, None, None]], 'uris': None, 'data': None, 'metadatas': [[{'chunk_id': 180, 'text': 'be some mutual respect (which there is). Or maybe mutual\\ntrust is what to look for (and it’s there). Perhaps these\\nmetrics would be better assessments of brotherhood than\\nhow many text messages he and I exchange.\\nThis clearly makes sense; it feels true for me. But it still\\nfucking hurts that my brother and I aren’t close. And there’s\\nno positive way to spin it. There’s no secret way to glorify\\nmyself through this knowledge. Sometimes brothers—even\\nbrothers who love each other—don’t have close\\nrelationships, and that’s ﬁne. It is hard to accept at ﬁrst, but\\nthat’s ﬁne. What is objectively true about your situation is\\nnot as important as how you come to see the situation, how\\nyou choose to measure it and value it. Problems may be\\ninevitable, but the meaning of each problem is not. We get\\nto control what our problems mean based on how we\\nchoose to think about them, the standard by which we\\nchoose to measure them.\\nRock Star Problems'}, {'chunk_id': 71, 'text': 'problem by buying a gym membership, you create new\\nproblems, like having to get up early to get to the gym on\\ntime, sweating like a meth-head for thirty minutes on an\\nelliptical, and then getting showered and changed for work\\nso you don’t stink up the whole oﬃce. When you solve your\\nproblem of not spending enough time with your partner by\\ndesignating Wednesday night “date night,” you generate\\nnew problems, such as ﬁguring out what to do every\\nWednesday that you both won’t hate, making sure you have\\nenough money for nice dinners, rediscovering the chemistry\\nand spark you two feel you’ve lost, and unraveling the\\nlogistics of fucking in a small bathtub ﬁlled with too many\\nbubbles.\\nProblems never stop; they merely get exchanged and/or\\nupgraded.\\nHappiness comes from solving problems. The keyword\\nhere is “solving.” If you’re avoiding your problems or feel\\nlike you don’t have any problems, then you’re going to\\nmake yourself miserable. If you feel like you have problems'}, {'chunk_id': 9, 'text': 'person feels the need to stand in front of a mirror and recite\\nthat she’s happy. She just is.\\nThere’s a saying in Texas: “The smallest dog barks the\\nloudest.” A conﬁdent man doesn’t feel a need to prove that\\nhe’s conﬁdent. A rich woman doesn’t feel a need to\\nconvince anybody that she’s rich. Either you are or you are\\nnot. And if you’re dreaming of something all the time, then\\nyou’re reinforcing the same unconscious reality over and\\nover: that you are not that.\\nEveryone and their TV commercial wants you to believe\\nthat the key to a good life is a nicer job, or a more rugged\\ncar, or a prettier girlfriend, or a hot tub with an inﬂatable\\npool for the kids. The world is constantly telling you that the\\npath to a better life is more, more, more—buy more, own\\nmore, make more, fuck more, be more. You are constantly\\nbombarded with messages to give a fuck about everything,\\nall the time. Give a fuck about a new TV. Give a fuck about\\nhaving a better vacation than your coworkers. Give a fuck'}]], 'distances': [[5790.923828125, 6550.193359375, 7083.81884765625]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "# Ensure the embedding model used for querying is the same as the one used for creating the collection\n",
    "query_embedding = embed_model_ollama([\"happiness is a problem\"])\n",
    "\n",
    "# Query the collection to retrieve all documents\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding,  # Use the embedding of the query\n",
    "    n_results=3  # Number of documents to retrieve\n",
    ")\n",
    "\n",
    "print(f\"Found {len(results)} results.\", results )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Use the existing client and collection\n",
    "vectorstore = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"sangf_rag\",\n",
    "    embedding_function=embed_model_ollama,\n",
    "    persist_directory=\"chroma_db\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OllamaEmbeddingFunction' object has no attribute 'embed_query'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Embed the query\u001b[39;00m\n\u001b[0;32m      4\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m embed_model_ollama([query])\n\u001b[1;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# the search query embedding\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# returns top 3 most relevant chunks of text\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\vectorstores\\chroma.py:350\u001b[0m, in \u001b[0;36mChroma.similarity_search\u001b[1;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    335\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m        List[Document]: List of documents most similar to the query text.\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\vectorstores\\chroma.py:439\u001b[0m, in \u001b[0;36mChroma.similarity_search_with_score\u001b[1;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[0;32m    432\u001b[0m         query_texts\u001b[38;5;241m=\u001b[39m[query],\n\u001b[0;32m    433\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    437\u001b[0m     )\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m(query)\n\u001b[0;32m    440\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[0;32m    441\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39m[query_embedding],\n\u001b[0;32m    442\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    446\u001b[0m     )\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OllamaEmbeddingFunction' object has no attribute 'embed_query'"
     ]
    }
   ],
   "source": [
    "query = 'Happiness Is a Problem'\n",
    "\n",
    "# Embed the query\n",
    "query_embedding = embed_model_ollama([query])\n",
    "\n",
    "results = vectorstore.similarity_search(\n",
    "    query_embedding,  # the search query embedding\n",
    "    k=3  # returns top 3 most relevant chunks of text\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
